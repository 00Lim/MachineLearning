## 09. Neural Networks - Learning

#### <u>Neural network cost function</u>

Neural network는 가장 강력한 learning 알고리즘 중에 하나이다.

- Training set에서 파생된 매개변수들을 맞추는 알고리즘이다.
- L - network에서 layer의 개수
- S<sub>1</sub> - layer 1에서의 unit의 개수

<br>

<b>Types of classification problems with NNs</b>

NNs는 두 가지로 분류할 수 있다.

- Binary classification
  - output이 0과 1이다.
  - 그래서 output node가 하나이다. (S<sub>l</sub> = 1)
- Multi-class classification
  - 3이상의 수인 k개로 분류한다.
  - S<sub>l</sub> = k

<br>

<b>Cost function for neural networks</b>

logistic regression의 cost function은 아래와 같다.
$$
J(\theta) = -\frac{1}{m}\left[ \sum_{i=1}^{m} y^{(i)} \log h_\theta(x^{(i)}) + (1-y^{(i)}) \log (1-h_\theta(x^{(i)})) \right]+\frac{\lambda}{2m} \sum_{j=1}^{n}\theta_j^2
$$
neural network의 경우는 출력으로 하나의 값 대신 k개의 값이 나온다.

따라서 cost function은...
$$
J(\theta) = -\frac{1}{m}\left[ \sum_{i=1}^{m} \sum_{k=1}^{K} y_k^{(i)} \log( h_\theta(x^{(i)}))_k + (1-y^{(i)}) \log (1-(h_\theta(x^{(i)}))_k) \right]+\frac{\lambda}{2m} \sum_{l=1}^{L-1} \sum_{i=1}^{s_l} \sum_{j=1}^{s_{l+1}} (\theta_{ij}^{(l)})^2
$$
이제 cost function의 output은 크기가 k인 vector이다.

 h<sub>θ</sub>(x)가 크기가 k인 vector이므로 h<sub>θ</sub>(x)<sub>i</sub>는 벡터의 i번째 요소이다.

<br>

<b>First half</b>

$$
-\frac{1}{m}\left[ \sum_{i=1}^{m} \sum_{k=1}^{K} y_k^{(i)} \log( h_\theta(x^{(i)}))_k + (1-y^{(i)}) \log (1-(h_\theta(x^{(i)}))_k) \right]
$$
이 부분은 모든 training data에 대해 logistic regression의 결과의 평균을 구하는 부분이다.

<br>

<b>Second half</b>

$$
\frac{\lambda}{2m} \sum_{l=1}^{L-1} \sum_{i=1}^{s_l} \sum_{j=1}^{s_{l+1}} (\theta_{ij}^{(l)})^2
$$
이 부분은 정규화 부분이다. weight의 비중을 감소시키는 역할을 한다.

<br>

<br>

#### <u>Summary of what's about to go down</u>

Back propagation

- neural network의 출력값과 실제 값을 비교해보고 매개변수들이 얼마나 잘못된 값을 가지고 있는지 계산하는 방법이다.
- 출력 값을 이용해 계산한 오류를 이용해서 그 전 layer에 있는 매개변수들의 오류를 계산한다.
  - input layer에 도달할 때까지 반복한다.
- 오류의 계산은 편미분을 이용해서 할 수 있다.
- cost function을 최소화하고 모든 매개변수를 업데이트 하기 위해 경사 하강법과 편미분을 같이 사용한다.
  - 경사 하강법의 값이 수렴할 때까지 반복한다.

<br>

<br>

#### <u>Back propagation algorithm</u>

편미분을 사용해서 cost function이 최소화되게 만들 것이다.

<br>

<b>What is back propagation?</b>

δ<sub>j</sub><sup>l</sup>은 l layer에 있는 j node의 오류를 나타낸다.

Activation node의 값은 이미 계산된 값이기 때문에 오류의 값도 실수이다.

Output layer에 있는 값에서부터 계산을 시작한다.

<br>

#### <u>Back propagation</u>

편미분을 통해 파라미터의 값을 찾았다면 그 값을 바로 업데이트 하면 안된다!

계산한 값이 계속 쓰이기 때문에 미리 업데이트 시켜주면 input node에 도달할수록 값이 이상하게 나온다.

모든 파라미터의 값을 찾은 다음 동시에 업데이트 시켜줘야한다.

<br>

![smallNN](C:\Users\dladu\Desktop\smallNN.png)

다음의 작은 neural network를 가지고 한번 살펴보자.

먼저 w<sub>10</sub><sup>(1)</sup>의 값을 다시 계산을 해보면

$$
\frac{\partial E_{tot}}{\partial w_{10}^{(1)}} = \frac{\partial E_{tot}}{\partial a_{20}} \frac{\partial a_{20}}{\partial z_{20}} \frac{\partial z_{20}}{\partial w_{10}^{(1)}}
$$

위 식을 통해 나온 값으로

$$
w_{10}^{(1)} := w_{10}^{(1)} - \alpha * \frac{\partial E_{tot}}{\partial w_{10}^{(1)}}
$$

gradient descent 식을 이용해서 w<sub>10</sub><sup>(1)</sup>의 값을 구할 수 있다.

위와 같이 모든 파라미터들의 값을 구하고 동시에 업데이트 해준다.

<br>

w<sub>10</sub><sup>(1)</sup>은 output node 하나에만 영향을 끼쳤다.

또 다른 예시로 output node 여러개의 영향을 끼치는 값을 back propagation으로 고쳐보자.

output node 두개에 영향을 끼치는 파라미터인 w<sub>10</sub><sup>(0)</sup>의 값을 계산해보면

$$
E_{tot} = E_{y1} + E_{y2},\quad \frac{\partial E_{tot}}{\partial w_{10}^{(0)}} = \left( \frac{\partial E_{y1}}{\partial a_{10}} + \frac{\partial E_{y1}}{\partial a_{10}} \right) \frac{\partial a_{10}}{\partial z_{10}} \frac{\partial z_{10}}{\partial w_{10}^{(0)}}
$$

이고 E<sub>tot</sub>는 두 노드의 오류값의 합이므로 E<sub>y1</sub>을 먼저 계산한다.

$$
\frac{\partial E_{y1}}{\partial a_{10}} = \frac{\partial E_{y1}}{\partial a_{20}} \frac{\partial a_{20}}{\partial z_{20}} \frac{\partial z_{20}}{\partial a_{10}}
$$

그 다음 E<sub>y2</sub>의 값을 계산한다.

$$
\frac{\partial E_{y2}}{\partial a_{10}} = \frac{\partial E_{y2}}{\partial a_{21}} \frac{\partial a_{21}}{\partial z_{21}} \frac{\partial z_{21}}{\partial a_{10}}
$$

계산한 두 값을 이용해서 w<sub>10</sub><sup>(0)</sup>의 값을 업데이트 해준다.

$$
w_{10}^{(0)} := w_{10}^{(0)} - \alpha * \frac{\partial E_{tot}}{\partial w_{10}^{(0)}}
$$
