## Regression Analysis and Gradient Descent

#### 머신러닝 알고리즘

+ Supervised Learning
  + Regression Problem
  + Classfication Problem
+ Unsupervised Learning

<br>

##### Supervised Learning

정답이 있는 데이터들을 받은 알고리즘이 새로운 데이터들의 결과를 도출한다.

알고리즘이 도출한 결과와 실제 결과를 비교하여 피드백 후 알고리즘을 발전시킬 수 있다.

도출한 결과들에 따라 Regression problem과 Classification problem으로 나뉜다.

Regression problem은 도출되는 결과들이 연속되는 값(실수)을 가지고 있고

Classification problem은 도출되는 결과들이 이산적인 값을 가지고 있다.

<br>

##### Unsupervised Learning

알고리즘에게 데이터들을 주고 분류하게 한다.

알고리즘에게 문제를 던져줬지만 결과는 우리도 모른다.

잘못된 분류를 고쳐줄 수 없고 피드백을 통한 알고리즘의 발전도 불가능하다.

<br>

##### Cost Function

계산된 결과값에서 알고리즘이 추측한 결과값을 뺀 값, 오류이다.

최소화하는게 목적이다.

<br><br>

#### Gradient Descnet Alogrithm

Cost Function J의 값을 최소하기 위한 알고리즘이다.

머신러닝의 최소화에 전반적으로 사용된다.

1. (0, 0) 또는 아무곳에서나 시작
2. 미분을 이용해서 J의 값을 줄이는 값으로 매개변수들의 값을 조금씩 바꾼다.
3. 최솟값에 도달할 때 까지 반복한다.
4. 시작하는 지점에 따라 도달하는 최솟값이 다를 수 있다.

매개변수들이 동시에 업데이트 되어야한다!

따라서 매개변수들의 값을 저장할 temp값이 필요하다.

동시에 업데이트 되지 않으면  Gradient Descent가 아니며 결과도 이상하게 나온다.

<br>

##### Learning Rate 알파 값에 대해

- 알파 값이 너무 작으면

  ​	너무 많은 실행 횟수를 가지게 된다.

- 알파 값이 너무 크면

  ​	최솟값을 넘어가서 최솟값에 도달하지 못할 수도 있다.

- Local minimum에 도달했을 때

  ​	기울기는 0 -> 미분한 도함수가 0 -> 알파 * 0 = 0 -> 세타의 값은 변함이 없음.

- Global minimum

  ​	Global minimum에 도달할수록 도함수의 값아 작아진다.

  ​	도함수의 값이 작아짐에 따라 (알파 * 도함수)의 값이 점점 작아진다.

  ​	매번 알파 값을 수정하지 않아도 된다.
