## Recommender Systems

#### <u>Recommender systems - introduction</u>

추천 시스템은 오늘날에 가장 많이 사용되는 알고리즘이다.

다양한 분야의 서비스에서 사용되고 있으며 대표적으로는 아마존과 같은 쇼핑 사이트에서 사용되고 있다.

한 가지 재미있는 점은 추천서비스는 학문적으로는 크게 주목을 받지 않을 만큼 기술적인 특색이 없지만, 상업적으로는 아주 중요한 서비스로 인식이 되고 있다는 것이다.

기술보다는 아이디어의 차이가 추천 서비스의 핵심이라고 할 수 있다.

<br>

<b>Example - predict movie ratings</b>

4명의 사용자가 5개의 영화에 대해 평가하도록 한다.

?는 해당 영화를 평가하지 않았다는 의미이다.

![](http://www.holehouse.org/mlclass/16_Recommender_Systems_files/Image.png)

- n<sub>u</sub> - 사용자의 수
- n<sub>m</sub> - 영화의 수
- r(i, j) - 사용자j가 영화 i를 평가했다면 1
- y<sup>(i, j)</sup> - 사용자 j가 영화i에 부여한 점수

Alice와 Bob은 로맨스 영화에 높은 점수를 주었고 액션 영화에는 낮은 점수를 주었다.

반대로 Carol과 Dave는 액션 영화에 높은 점수를 주었고 로맨스 영화에 낮은 점수를 주었다.

이를 통해서 ?에 들어갈 값을 예측할 수 있다.

<br>

<br>

#### <u>Content base recommendation</u>

위 예시에서 영화의 로맨스와 액션 양을 계산한 값을 feature로 넣어보자.

- 로맨스(x<sub>1</sub>)
- 액션(x<sub>2</sub>)

위의 영화들에 두가지 feature 값을 넣어보자.

![](http://www.holehouse.org/mlclass/16_Recommender_Systems_files/Image%20[1].png)

첫 번째 영화인 Love at last의 feature 벡터를 보면 x<sub>0</sub>은 bias로 1의 값을 갖는다.

- x<sup>(1)</sup> = [1, 0.9, 0]

이렇게 x<sup>1</sup>부터 x<sup>5</sup>까지의 data set이 만들어진다.

또, 각 사용자에 대한 파라미터를 θ로 놓으면 이 θ는 사용자의 성향도를 나타내는 벡터가 된다.

- θ<sup>(1)</sup> = [0, 5, 0]

따라서 영화의 feature 벡터와 사용자의 θ벡터를 곱하면 별점 정보가 된다.

- (θ<sup>j</sup>)<sup>T</sup> * x<sup>i</sup> = stars

이제 Alice가 평가를 하지 않았던 Cute puppies of love의 별점 정보를 예측해보자.

- (θ<sup>(1)</sup>)<sup>T</sup> * x<sup>3</sup> = (0 * 1) + (5 * 0.99) + (0 * 0) = 4.95

<br>

<b>How do we learn (θ<sup>j</sup>)</b>

이것은 linear regression과 비슷하다.

그래서 우리는 θ<sup>j</sup>를 학습하기 위해 다음의 식을 사용한다

![](http://www.holehouse.org/mlclass/16_Recommender_Systems_files/Image%20[7].png)

하지만 우리는 모든 사용자의 파라미터를 학습해야한다.

![](http://www.holehouse.org/mlclass/16_Recommender_Systems_files/Image%20[8].png)

그리고 우리는 위의 식을 최소화 하는것이 최적화 목표이다.

이것은 사용자 j가 평가를 한 영화i에 대해서만 수행을 한다.

최소화 하기 위해 다음과 같은 경사하강법이 있다.

![](http://www.holehouse.org/mlclass/16_Recommender_Systems_files/Image%20[9].png)

이전의 경사하강법과는 살짝 다르다.

k = 0일때와 k != 0일때가 있다.

정규화를 진행할 때 bias를 제외한 나머지 값들로만 진행을 하기 위해서이다.

<br>

<br>

#### <u>Collaborative filtering - overview</u>

협업 필터링 알고리즘은 학습해야 하는 feature를 스스로 학습할 수 있다.

우리가 위에서 했던 영화의 장르에 대한 feature는 실제로 하기에는 매우 어려울 수 있다.

따라서 영화와 관련된 feature를 모르는 data set이 있다고 가정할 수 있다.

![](http://www.holehouse.org/mlclass/16_Recommender_Systems_files/Image%20[11].png)

이제 다른 가정으로 사용자가 로맨스 영화와 액션 영화를 얼만큼 좋아하는지 알아냈다.

![](http://www.holehouse.org/mlclass/16_Recommender_Systems_files/Image%20[12].png)

이러한 매개변수들로 영화와 관련된 feature를 채울 수 있다.

영화 Love at last를 보면 로맨스를 좋아하는 Alice와 Bob이 높은 점수를 주었다.

또 로맨스를 싫어하는 Carol과 Dave가 낮은 점수를 주었다.

따라서 우리는 Love at last가 로맨스 영화라고 결론을 내릴 수 있다.

<br>

<b>Formalizing the collaborative filtering problem</b>

θ가 주어졌을 때

![](http://www.holehouse.org/mlclass/16_Recommender_Systems_files/Image%20[14].png)

이전과 마찬가지로 위의 식은 한 영화의 특징을 배우는 방법을 제공한다.

<br>

<b>How does this work with the previous recommendation system</b>

1. θ의 값을 무작위로 추측한다.
2. 이 값을 이용해서 x를 생성한다.
3. 이 x값을 이용해서 θ를 개선한다.
4. 그리고 다시 x를 개선한다...

이렇게 사용자가 평점을 주는 행위를 통해서 알고리즘이 학습할 수 있도록 도움을 주기 때문에 이것을 협업필터링이라고 한다.

<br>

<br>

#### <u>Collaborative filtering Algorithm</u>

우리는 다음과 같이 시작할 수 있다.

- 영화의 feature가 주어지면 사용자의 선호도를 알아낸다

  ![](http://www.holehouse.org/mlclass/16_Recommender_Systems_files/Image%20[15].png)

- 사용자의 선호도가 주어지면 영화의 feature를  알아낼 수 있다.

  ![](http://www.holehouse.org/mlclass/16_Recommender_Systems_files/Image%20[16].png)

자세히 살펴보면 위의 두 식은 비슷하다는 것을 알 수 있다.

따라서 위 두 식을 하나로 합쳐서 영화의 feature와 사용자의 선호도를 동시에 업데이트 할 수 있다.

![](http://www.holehouse.org/mlclass/16_Recommender_Systems_files/Image%20[17].png)

이러한 접근 방식을 사용하면 feature도 알고리즘이 스스로 학습하기 때문에 x<sub>0</sub> 항이 없어진다.

따라서 모든 벡터들은 n의 크기를 가진다.

<br>

<b>Algorithm Structure</b>

1. θ와 x를 임의의 작은 값으로 초기화 한다.

2. 경사 하강법을 사용하여 cost function을 최소화시킨다.

   - 업데이트 규칙은 다음과 같다.

     ![](http://www.holehouse.org/mlclass/16_Recommender_Systems_files/Image%20[19].png)

     ![](http://www.holehouse.org/mlclass/16_Recommender_Systems_files/Image%20[20].png)

   - 위의 식은 x에 대한 cost function의 편미분이고 아래 식은 θ에 대한 cost function의 편미분이다.

   - 여기서 모든 매개변수를 정규화한다.

3. 이렇게 학습하여 찾아낸 x feature와 θ 파라미터를 곱하여 사용자가 영화에 매길 평점을 예측할 수 있다.

<br>

<br>

#### <u>Vectorization: Low rank matrix factorization</u>

협업 필터링을 어떻게 개선할 수 있을까?

우리는 모든 사용자의 모든 평가를 행렬 Y로 나타낼 수 있다.

![](http://www.holehouse.org/mlclass/16_Recommender_Systems_files/Image%20[21].png)

영화가 5편이고 사용자가 4명이므로 [5 X 4] 행렬로 표현한다.

또 다른 행렬 x를 정의할 수 있다.

- 각 영화의 모든 feature를 가져와서 쌓는 것이다.

  ![](http://www.holehouse.org/mlclass/16_Recommender_Systems_files/Image%20[23].png)

또 다른 행렬 θ도 있다.

- 각 사용자의 모든 매개 변수를 가져와서 쌓는 것이다.

  ![](http://www.holehouse.org/mlclass/16_Recommender_Systems_files/Image%20[24].png)

우리는 새로운 행렬 x와 θ가 주어지면 x * θ<sup>T</sup>를 계산하여 평점 정보를 예측할 수있다.

이때 이 x행렬과 θ행렬을 low rank matrix라고 하고 이러한 방식의 알고리즘을 low rank matrix factorization이라고 한다.

<br>

<br>

#### <u>Implementation detail: Mean Normalization</u>

Mean normalization이 필요한 이유를 보여주기 위해 영화를 하나도 평가하지 않은 사용자가 있다고 하자.

![](http://www.holehouse.org/mlclass/16_Recommender_Systems_files/Image%20[25].png)

Eve는 평점을 남긴 영화가 없기 때문에 r(i, j)가 모두 0이 된다.

따라서 공식에는 정규화 부분만 남게 된다. 그리고 이것을 minimize하면 당연히 θ는 0이 된다.

결국 θ<sup>T</sup> * x도 0이 되므로 모든 영화의 예상 평점이 0이 되는 사태가 발생한다.

![](http://www.holehouse.org/mlclass/16_Recommender_Systems_files/Image%20[26].png)

이렇게 되면 평점 정보가 없는 신규 사용자에게도 다른 영화를 추천할 수 없는 문제가 발생한다.

이와 같은 문제를 해결하기 위해 mean normalization이 필요하다.

<br>

<b>How does mean normalization work?</b>

이전과 같이 모든 평점 정보를 행렬 Y로 나타내보자.

![](http://www.holehouse.org/mlclass/16_Recommender_Systems_files/Image%20[28].png)

우리는 열 벡터로부터 각 영화의 평균 평점 정보를 계산한다.

![](http://www.holehouse.org/mlclass/16_Recommender_Systems_files/Image%20[29].png)

그리고 Y행렬의 값에서 평균을 빼준다.

![](http://www.holehouse.org/mlclass/16_Recommender_Systems_files/Image%20[30].png)

이렇게 영화의 평점정보의 평균이 0이 되도록 정규화 해준다.

그리고 영화 i에서 사용자 j에 대한 예측을 하기 위해

- (θ<sup>j</sup>)<sup>T</sup> * x<sup>i</sup> + μ<sub>i</sub>

이렇게 계산하면 평점 정보가 없어서 (θ<sup>j</sup>)<sup>T</sup> * x<sup>i</sup>의 값은 0이 되지만 뒷부분의 평균 평점은 여전히 남아있다.

즉, 새로운 사용자나 평점 정보가 없는 사용자에게는 각 영화의 평균평점값이 적용되게 된다.

이것이 우리가 할 수 있는 최선의 방법이다.