## Linear Regression with Multiple Variables

#### Linear regression with multiple features

그 전까지는 변수 하나만 가지고 결과를 예측하였지만 변수 여러개를 가지고도 결과를 예측할 수 있다.

변수가 여러개가 되면 세타와 x를 벡터로 나타낸다.

따라서 함수 h를 세타 벡터와 x 벡터의 행렬곱 연산으로 나타낼 수 있다.

세타 벡터의 transpose는 [1 X n+1]의 크기를 가진 column벡터이고, x 벡터는 [n+1 X 1]의 크기를 가진 row 벡터이다.

같은 방향을 가진 벡터끼리는 곱 연산이 안되기 때문에 세타 벡터를 transpose 시켜주었다.

<br>

#### Gradient Descent in Practice : 1 Feature Scaling

모든 변수가 비슷한 범위에 있다면 gradient descent가 수렴하는데 더 도움이 된다.

꼭 같은 범위가 아니라 비슷한 범위여도 좋다.

비슷한 범위로 만들기 위해 입력값들의 수치를 조정할 수 있다.

- 입력값을 조정하는 방법 1
  1. 입력값들을 가장 큰 값으로 나눈다.
  2. 범위가 0~1로 좁혀지게 되므로 모양이 원에 가까워진다.
  3. 조금 더 효율적인 gradient descent가 되었다.
- 입력값을 조정하는 방법 2
  1. x의 값을 (x - 평균) / max 한 값으로 나눠준다.
  2. 값들의 평균이 0에 가까워진다.
  3. 조금 더 효율적인 gradient descent가 되었다.

<br>

#### Features and polynomial regression

꼭 주어진 변수만 사용할 필요 없다. 주어진 변수들을 이용해서 새로운 변수를 만들어서 사용해도 된다. 새로 만들어진 변수가 더 좋은 지표가 될 수 있다.

<br>

#### Normal equation

J(세타) = a(세타)<sup>2</sup> + b(세타) + c

위 식의 최솟값을 찾기 위해 세타로 미분한다.

미분한 도함수가 0이 되는 세타 값을 찾는다.

그 세타 값이 J(세타)의 최솟값이다.

<br>

#### When should you use gradient descent and when should you use feature scaling?

##### Gradient descent

- Learning rate 알파의 값이 필요할 때
- 많은 반복이 필요할 때
- n(feature)이 클 때

##### Normal equation

- Learning rate 알파가 필요하지 않을 때
- 반복할 필요가 없을 때
- n(feature)이 작을 때
  - Normal equation의 알고리즘의 효율은 O(n<sup>3</sup>)이기 때문에 feature의 수가 많을 수록 연산량이 기하급수적으로 늘어난다.