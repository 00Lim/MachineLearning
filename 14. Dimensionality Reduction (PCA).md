## Dimensionality Reduction (PCA)

#### <u>Motivation 1: Data compression</u>

<b>Compression</b>

데이터가 사용하는 공간을 줄임으로써 알고리즘의 속도가 향상된다.

예를 들면 서로 다른 단위를 사용하는 동일한 데이터를 하나로 줄일 수 있다.

![](http://www.holehouse.org/mlclass/14_Dimensionality_Reduction_files/Image%20[1].png)

위의 그래프처럼 2차원의 데이터를 1차원으로 표현할 수 있다.

![](http://www.holehouse.org/mlclass/14_Dimensionality_Reduction_files/Image%20[4].png)

위의 예시처럼 3차원에 있는 데이터들이 상대적으로 한 평면 위에 있다면 이것을 2차원으로 표현할 수도 있다.

<br>

<br>

#### <u>Motivation 2: Visualization</u>

데이터의 차원이 3차원을 넘으면 시각화하기 어렵다. 고차원 데이터들을 2차원이나 3차원으로 줄여 눈으로 확인하기 쉽게 도울때에도 사용한다.

![](http://www.holehouse.org/mlclass/14_Dimensionality_Reduction_files/Image%20[5].png)

이렇게 많은 데이터를 요약하는 새로운 feature를 찾는다.

![](http://www.holehouse.org/mlclass/14_Dimensionality_Reduction_files/Image%20[6].png)

굉장히 많은 데이터를 2차원으로 줄이는것에 성공했다.

물론 이러한 데이터들은 기존의 데이터들의 특징을 가지고 있어야 한다.

<br>

<br>

#### <u>Principle Component Analysis (PCA): Problem Formulation</u>

차원 감소가 가장 일반적으로 사용되는 알고리즘은 PCA이다.

아래의 2차원 데이터를 1차원으로 축소한다고 가정하자.

![](http://www.holehouse.org/mlclass/14_Dimensionality_Reduction_files/Image%20[7].png)

먼저 이 데이터들을 투영할 하나의 선을 찾는다.

- 각 지점과 투영된 선 사이의 거리는 짧아야 한다.
- PCA는 더 낮은 차원을 찾으려고 하므로 해당 차원의 제곱합이 최소화 된다.

![](http://www.holehouse.org/mlclass/14_Dimensionality_Reduction_files/Image%20[8].png)

위 그림의 파란 선을 투영 오류라고 한다.

PCA는 이 투영 오류가 최소화 되는 직선을 찾으려고 하는 것이다.

<br>

PCA와 linear regression은 비슷해보이지만 다르다.

linear regression은 각 데이터와 prediction line간의 squared error를 최소화한다.

PCA에서는 각 데이터와 projection line간의 shortest distance를 최소화한다.

![](https://wikidocs.net/images/page/4870/dim202.PNG)

즉, linear regression은 y를 예측하기 위해 x의 모든 example을 택해서 세타의 파라미터를 적용한다. 하지만 PCA에서는 feature들 사이에 가장 가까운 공통적인 데이터셋을 찾는다.

PCA에서는 어떤 결과를 예측하지도, 파라미터를 조정하지도 않는다.

<br>

<br>

#### <u>PCA Algorithm</u>

PCA를 적용하기 전에 데이터 전처리를 해야한다.

- 평균 정규화
  - 각 x<sub>j</sub><sup>i</sup>를 x<sub>j</sub> - μ<sub>j</sub>로 바꾼다.
    - 즉, 각 feature 집합의 평균을 구해서 각 feature의 값에다가 평균을 빼서 평균을 0으로 만든다.
- Feature scaling(데이터에 따라 다름)
  - feature들이 비슷한 값을 갖도록 해준다.
    - x<sub>j</sub><sup>i</sup>는 (x<sub>j</sub> - μ<sub>j</sub>) / s<sub>j</sub>로 바꿔준다.

전처리가 완료되면 PCA는 제곱의 합을 최소화하는 더 낮은 차원의 공간을 찾는다.

2차원에서 1차원으로 줄일 경우는 다음과 같이 할 것이다.

![](http://www.holehouse.org/mlclass/14_Dimensionality_Reduction_files/Image%20[11].png)

두 가지를 계산해야 한다.

- u 벡터 계산
- z 벡터 계산

<br>

<b>Algorithm description</b>

n차원에서 k차원으로 데이터 줄이기

1. 공분산 행렬 계산

![](http://www.holehouse.org/mlclass/14_Dimensionality_Reduction_files/Image%20[12].png)

2. x<sup>(i)</sup>는 n X 1 벡터이고 (x<sup>(i)</sup>)<sup>T</sup>는 1 X n 벡터이므로 계산 결과는 n X n 행렬이다.

3. 공분산 행렬을 이용해 고유벡터(eigenvectors)를 계산한다.
   [U, S, V] = svd(Sigma)



![](http://www.holehouse.org/mlclass/14_Dimensionality_Reduction_files/Image%20[14].png)

4. U 벡터의 크기는 n X n이다.
   n차원에서 k차원으로 줄이려면 U 행렬의 처음부터 k번째 벡터까지 가져온다.
5. z = (U<sub>reduce</sub>)<sup>T</sup> * x

<br>

<br>

#### <u>Reconstruction from Compressed Representation</u>

이번엔 낮은차원으로 압축된 데이터를 다시 압축 해제 하는 방법을 알아보자.

다음과 같이 압축한 데이터가 있다고 가정하면

![](http://www.holehouse.org/mlclass/14_Dimensionality_Reduction_files/Image%20[15].png)

위의 z를 구하는 식을 이용해 x를 구할 수 있다.

x<sub>approx</sub> = U<sub>reduce</sub> * z

![](http://www.holehouse.org/mlclass/14_Dimensionality_Reduction_files/Image%20[16].png)

물론 실제 x와 복원된 x는 다르기 때문에 복원된 x를 x<sub>approx</sub>라고 한다.

<br>

<br>

#### <u>Choosing the number of Principle Components</u>

그렇다면 k는 어떻게 결정할까

PCA는 평균 제곱 투영 오차를 최소화 하려고 한다.

![](http://www.holehouse.org/mlclass/14_Dimensionality_Reduction_files/Image%20[19].png)

따라서 k를 선택할 때 위의 식을 이용해서 적절한 k의 값을 찾을 수 있다.

<br>

<br>

#### <u>Advice for Applying PCA</u>

<b>Speeding up supervised learning algorithms</b>

feature의 수가 아주 많은 supervised learning의 경우 연산 속도가 아주 느릴 것이다.

이러한 경우에 PCA를 사용하여 차원을 줄이고 연산 속도를 높일 수 있다.

일반적으로 알고리즘에 큰 타격을 주지 않고도 데이터 차원을 5 ~ 10배나 줄일 수 있다.

<br>

<br>

#### <u>Applications of PCA</u>

Compression

- 데이터를 저장할 메모리/디스크 감소
- 알고리즘의 속도 향상

Visualization

- 일반적으로 k = 2 또는 3을 선택한다.
- 따라서 데이터들의 분포를 그릴 수 있게 된다.

PCA를 잘못 사용하는 경우

- over fitting을 방지하기 위해서 사용
  - 잘 작동하지 않는다.
  - 잘 작동할 때도 있지만 over fitting 문제를 해결하기에는 정규화가 더 좋다.

종종 ML을 설계할 때 처음부터 PCA를 사용하여 시스템을 설계할 때가 있다.

PCA는 중간에 쉽게 추가할 수 있으므로 먼저 시도하지 말자.