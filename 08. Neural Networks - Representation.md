## Neural Networks - Representation

#### <u>Neural networks - Overview and summary</u>

<b>Why do we need neural networks?</b>

linear regression이나 logistic regression으로는 복잡한 데이터를 다루는데 한계가 있다.

<br>

<b>Neurons and the brain</b>

Neural network는 원래 우리의 뇌의 기능을 따라하는 기계에서부터 시작했다.

우리의 두뇌는 스스로 학습하는 방법을 배운다.

<br>

<br>

#### <u>Model representation I</u>

Neuron은 우리의 신경망을 시뮬레이션 하는 방법으로 발전했다.

우리의 신경망은 하나 이상의 입력을 받아서 처리하고 출력한다.

<br>

<b>Artificial neural network - representation of a neurone</b>

Neuron은 하나의 logistic unit이다.

- 입력 와이어를 통해 feed를 입력받는다.
- logistic unit은 계산한다.
- 계산 결과를 출력 와이어로 보낸다.

첫 번째 레이어는 input layer, 마지막 레이어는 output layer라고 한다.

Input layer와 output layer 사이에 있는 모든 레이어를 hidden layer라고 한다.

Hidden layer는 여러개가 있을 수도 있고 아예 없을 수도 있다.

<br>

<b>Neural networks - notation</b>

a<sub>i</sub><sup>j</sup> - j번째 layer에 있는 i번째 유닛

세타<sup>j</sup> - j번째 layer에서 j+1번째 layer로 전달되는 weight가 담긴 matrix

layer j가 S<sub>j</sub>개의 유닛들을 가지고 있고, layer j+1이 S<sub>j+1</sub>개의 유닛들을 가지고 있다면 세타<sup>j</sup>의 크기는 (S<sub>j</sub>) X (S<sub>j+1</sub>) + 1이다. (bias unit 때문에 1만큼 더해진다.)

모든 입력은 다음 layer의 모든 노드들로 이동한다.

<br>

<br>

#### <u>Model representation II</u>

계산을 효율적으로 하는 방법과 복잡한 비선형 함수를 학습하는 방법

- forward propagation

  - 입력부터 출력까지 순서대로 계산해서 저장

  - z<sub>1</sub><sup>2</sup> = θ<sub>10</sub><sup>1</sup>X<sub>0</sub> + θ<sub>11</sub><sup>1</sup>X<sub>1</sub> + θ<sub>12</sub><sup>1</sup>X<sub>2</sub> + θ<sub>13</sub><sup>1</sup>X<sub>3</sub>
  - a<sub>1</sub><sup>2</sup> = g(z<sub>1</sub><sup>2</sup>)

<br>

<b>Neural networks learning its own features</b>

Neural network의 마지막 노드는 logistic regression노드이다.

차이점은 마지막 노드의 input은 실제 input이 아닌 hidden layer에서 계산된 값이라는 점이다.

<br>

<br>

#### <u>Neural network example - computing a complex, nonlinear function of the input</u>

<b>Neural Network example: AND function</b>

파라미터 θ의 값

- θ<sub>10</sub><sup>1</sup> = -30
- θ<sub>11</sub><sup>1</sup> = 20
- θ<sub>12</sub><sup>1</sup> = 20

위의 값을 이용해 계산을 해보면

| <i>x</i><sub>1</sub> | <i>x</i><sub>2</sub> | h<sub>θ</sub>(<i>x</i>) |
| -------------------- | -------------------- | ----------------------- |
| 0                    | 0                    | g(-30) = 0(과 비슷)     |
| 0                    | 1                    | g(-10) = 0(과 비슷)     |
| 1                    | 0                    | g(-10) = 0(과 비슷)     |
| 1                    | 1                    | g(1) = 1(과 비슷)       |

<br>

이와 같은 방식으로 여러 함수들을 neural network로 구현할 수 있다. 