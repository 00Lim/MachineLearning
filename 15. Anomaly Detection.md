## Anomaly Detection

#### <u>Anomaly detection - problem motivation</u>

이상 탐지는 비정상적인 데이터를 찾는다.

예를 들어 아래와 같은 데이터가 있다고 하자.

![](http://www.holehouse.org/mlclass/15_Anomaly_Detection_files/Image.png)

위의 데이터에 새로운 데이터가 추가 되었다.

![](http://www.holehouse.org/mlclass/15_Anomaly_Detection_files/Image%20[1].png)

위의 데이터는 정상적인 데이터라고 볼 수 있다.

또 다른 데이터가 추가되었다.

![](http://www.holehouse.org/mlclass/15_Anomaly_Detection_files/Image%20[2].png)

위의 데이터는 다른 데이터들과 달리 혼자 떨어져 있는 것을 볼 수 있다.

이런 경우 위의 데이터를 이상 데이터라고 하고 이러한 데이터들을 찾는 방법에 대해 알아볼 것이다.

우리는 새로운 모델 p(x)를 사용할 것이다. 이 p(x)는 데이터들의 밀도를 이용한다.

- if p(x<sub>test</sub>) < ε
  - 이상 데이터이다.
- if p(x<sub>test</sub>) >= ε
  - 정상 데이터이다.

![](http://www.holehouse.org/mlclass/15_Anomaly_Detection_files/Image%20[3].png)

<br>

<b>Applications</b>

- 사기 감지
  - 접속시간, 위치정보, 로그인 여부
- 모니터링
  - 데이터 센터에 있는 컴퓨터들을 모니터링하여 메모리, 디스크, CPU 등에 문제가 있는지 점검

<br>

<br>

#### <u>The Gaussian distribution (optional)</u>

가우스 분포는 정규 분포라고도 불린다.

- μ - 평균 (그래프의 위치 결정)
- σ<sup>2</sup> - 분산 (그래프의 폭 결정)

평균과 분산에 따라 그래프의 모양이 달라진다. 하지만 그래프의 면적은 항상 1이다.

![](http://www.holehouse.org/mlclass/15_Anomaly_Detection_files/Image%20[6].png)

<br>

<b>Parameter estimation problem</b>

m개의 데이터 예제 세트의 분포가 아래와 같다.

![](http://www.holehouse.org/mlclass/15_Anomaly_Detection_files/Image%20[7].png)

이 데이터에서 가우스 분포를 찾을 수 있다.

![](http://www.holehouse.org/mlclass/15_Anomaly_Detection_files/Image%20[8].png)

이제 μ와 σ<sup>2</sup>의 값을 알아낼 수 있다.

- μ = 데이터들의 평균

- σ<sup>2</sup> = 표준 편차의 제곱

  ![](http://www.holehouse.org/mlclass/15_Anomaly_Detection_files/Image%20[9].png) 

<br>

<br>

#### <u>Anomaly detection algorithm</u>

x 데이터에 대한 training set이 있다.

x는 각각 n개의 feature마다 고유한 분포를 가지고 있을 것이다.

각 feature마다 μ와 σ<sup>2</sup>을 구한다.

모든 feature의 가우스 분포를 곱하면 그것이 p(x)가 된다.

- p(x) = p(x<sub>1</sub>; μ<sub>1</sub>, σ<sub>1</sub><sup>2</sup>) * p(x<sub>2</sub>; μ<sub>2</sub>, σ<sub>2</sub><sup>2</sup>) * ... * p(x<sub>n</sub>; μ<sub>n</sub>, σ<sub>n</sub><sup>2</sup>)

- ![](http://www.holehouse.org/mlclass/15_Anomaly_Detection_files/Image%20[10].png) 

<br>

<b>Algorithm</b>

1. Chose feature
   - 특이하게 값이 크거나 작거나 하여 이상이 생길만한 feature를 선택한다.
   - 또는 일반적인 분포를 가지고 있는 feature를 선택한다.
2. Fit parameters
   - 각 example에 대해 μ와 σ<sup>2</sup>를 구한다.
3. compute p(x)
   - 선택된 값에 대해 p(x)를 구한다.
   - p(x) < ϵ이면 이상값이다.

<br>

<b>Anomaly detection example</b>

예시를 하나 들어보자.

![](http://www.holehouse.org/mlclass/15_Anomaly_Detection_files/Image%20[12].png)

위와 같은 데이터가 있다.

x1과 x2에 대해 가우스 분포를 그려보면 다음과 같다.

![](http://www.holehouse.org/mlclass/15_Anomaly_Detection_files/Image%20[13].png)

위의 두 값을 곱하면 다음과 같은 분포가 나온다.

p(x)는 x1, x2가 어떠한 값을 가지고 있을 때 아래의 그래프에서의 높이를 나타낸다.

![](http://www.holehouse.org/mlclass/15_Anomaly_Detection_files/Image%20[14].png)

예시를 들어보자.

ε = 0.02이고 x<sup>1</sup><sub>test</sub>와 x<sup>2</sup><sub>test</sub>의 (x1, x2)좌표가 각각 (4, 4), (7, 1)이라고 한다.

- x<sup>1</sup><sub>test</sub>
  - x1의 가우스 분포에서 4의 높이 = 약 0.17
  - x2의 가우스 분포에서 4의 높이 = 약 0.25
  - p(x<sup>1</sup><sub>test</sub>) = 0.17 * 0.25 = 약 0.0425 >= ε
  - x<sup>1</sup><sub>test</sub>은 정상적인 값이다.
- x<sup>2</sup><sub>test</sub>
  - x1의 가우스 분포에서 7의 높이 = 약 0.1
  - x2의 가우스 분포에서 1의 높이 = 약 0.02
  - p(x<sup>2</sup><sub>test</sub>) = 0.1 * 0.02 = 약 0.002 < ε
  - x<sup>2</sup><sub>test</sub>은 이상값이다.

<br>

<br>

#### <u>Developing and evaluation and anomaly detection system</u>

지금까지 머신 러닝 알고리즘을 평가하는 방법으로는 실수를 반환하게 하여 사용하였다.

변경사항이 알고리즘에 미친 영향을 보기 위해 단일 숫자를 반환하면 알고리즘을 쉽게 평가할 수 있다.

Anomaly Detection System을 만들때도 이러한 방법이 유용하게 사용된다.

지금까지 살펴본 anomaly data들을 unlabeled data였다. 이것이 labled data가 된다면 평가하는 것이 가능할것이다.

그래서 특정 데이터가 anomaly data라면 y = 1로 정의하고 그렇지 않고 정상인 데이터라면 y = 0으로 하여 label을 생성할것이다.

<br>

비행기 엔진으로 예를 들어보자.

10000개의 정상적인 엔진이 있고 20개의 비정상적인 엔진이 있다.

- training set: 6000개의 정상적인 엔진
- cv set: 2000개의 정상적인 엔진과 10개의 비정상적인 엔진
- test set: 2000개의 정상적인 엔진과 10개의 비정상적인 엔진

비율이 약 3:1:1이다. cv set와 test set은 서로 다른 데이터를 이용해야 한다.

이렇게 비정상적인 엔진을 예측하는 알고리즘을 생각해보자.

우리는 데이터에 label을 붙여줬으므로 supervised learning처럼 보인다.

<br>

이제 이 알고리즘의 성능을 계산해보자.

y = 0인 데이터가 아주아주 많으므로 일반적인 방법으로 계산하면 안된다.

1. True positive, false positive, true negative, false negative를 계산한다.

2. Precision과 recall을 계산한다.

3. F1 score를 계산한다.

<br>

<br>

#### <u>Anomaly detection vs. supervised learning</u>

데이터에 label을 추가해서 supervised learning처럼 보일 수 있다.

하지만 왜 supervised learning을 사용하지 않는 것일까?

<br>

<b>Anomaly detection</b>

positive example이 매우 작다.

- 충분한 positive example을 학습하지 못한다.

negative example은 매우 크다.

- 이 negative example들은 p(x)를 구하는데만 사용된다.

많은 유형의 이상값이 있다.

- 이상값이 서로 다른 타입이어도 충분히 대응할 수 있다.

<br>

<b>Supervised learning</b>

충분히 많은 수의 normal, anormal 데이터가 필요하다.

- anormal 데이터를 학습하여 다른 anormal 데이터를 찾을 수 있다.

<br>

<br>

#### <u>Choosing features to use</u>

때때로 가우스 분포가 아닌 분포를 만날때도 있다.

아래의 그림과 같은 형태의 분포를 만났다고 하자.

![](http://www.holehouse.org/mlclass/15_Anomaly_Detection_files/Image%20[15].png)

이 데이터를 가우스 분포처럼 보이게 만들려면 모든 x에 대해서 log(x)를 취해주면 아래와 같아진다.

![](http://www.holehouse.org/mlclass/15_Anomaly_Detection_files/Image%20[16].png)

data set의 성향에 따라서 적절한 값들로 변형을 해주면 된다.

- x -> log(x)
- x -> log(x + c)
- x -> x<sup>1/2</sup>
- x -> x<sup>1/3</sup>

<br>

<b>Error analysis for anomaly detection</b>

supervised learning과 비슷하게 cv set을 이용해서 algorithm을 수행한다.

그 결과 중에서 잘못된 것을 분석하여 그것이 해결될 수 있는 새로운 feature를 생성한다.

가장 좋은 경우는 정상 데이터의 p(x)는 크고 비정상 데이터의 p(x)는 작은 것이다.

그러나 실제 상황에서는 정상과 비정상 데이터의 p(x)가 비슷한 범위에 존재할 수 있다.

이때는 anormal 데이터지만 p(x)가 큰 경우를 뽑아 분석하여 이들을 구별할 수 있는 새로운 feature를 고안한다.

![](https://wikidocs.net/images/page/4698/ano602.PNG)

<br>

<br>

#### <u>Multivariate Gaussian distribution</u>

Multivariate 가우스 분포는 가끔 가우스 분포 이상탐지가 실패하는 일부 이상값을 포착할 수 있는 기술이다.

![](http://www.holehouse.org/mlclass/15_Anomaly_Detection_files/Image%20[19].png)

왼쪽 그림에서 녹색의 x는 이상값이라고 확인이 된다.

하지만 오른쪽 그래프에서는 녹색의 x가 이상값인지 아닌지 잘 확인이 안된다.

우리는 이를 해결하기 위해 새로운 feature를 추가했었다.

이렇게 값이 이상값인지 아닌지 잘 확인이 되지 않는 이유는 다음과 같다.

![](http://www.holehouse.org/mlclass/15_Anomaly_Detection_files/Image%20[20].png)

위와 같이 분포가 원의 형태로 존재하기 때문에 오른쪽의 빨간 동그라미 친 데이터가 동일하게 판단된다.

이것을 보안하기 위해 나온 것이 multivariate 가우스 분포이다.

<br>

<b>Multivariate Gaussian distribution model</b>

Multivariate 가우스 분포는 지금까지의 가우스 분포와 달리 각각의 feature를 개별적으로 사용하는 대신 한번에 p(x)를 모델링한다.

매개변수

- μ - n차원 벡터
- Σ - [nXn]크기의 행렬 (공분산 행렬)

Multivariate 가우스 분포의 공식은 다음과 같다.

![](http://www.holehouse.org/mlclass/15_Anomaly_Detection_files/Image%20[29].png)

더 중요한 것은, p(x)가 어떤 모양일까

![](http://www.holehouse.org/mlclass/15_Anomaly_Detection_files/Image%20[23].png)일때

 ![](http://www.holehouse.org/mlclass/15_Anomaly_Detection_files/Image%20[24].png) 

![](http://www.holehouse.org/mlclass/15_Anomaly_Detection_files/Image%20[25].png)일때

![](http://www.holehouse.org/mlclass/15_Anomaly_Detection_files/Image%20[26].png) 위의 예제보다 조금 날카로워졌다.

이번엔 x2의 값은 고정시키로 x1의 값만 바꿔보자.

![](https://t1.daumcdn.net/cfile/tistory/2610D44757C2D01F25)

x2의 값이 고정된 상태에서 x1의 값이 작아지면 홀쭉한 모양이고 x1의 값이 커지면 뚱뚱한 모양이 된다.

이번엔 대각선이 0으로 고정되었던 값들을 바꿔보자.

![](https://t1.daumcdn.net/cfile/tistory/22057E4757C2D02135)

값들이 커질수록 대각선으로 홀쭉한 모양의 형태가 된다.

![](https://t1.daumcdn.net/cfile/tistory/2329A44757C2D0230F)

마이너스 값으로 커지면 반대 방향의 대각선으로 홀쭉한 모양이 된다.

이번엔 Σ의 값이 고정된 상태로 μ값을 바꿔보자.

![](https://t1.daumcdn.net/cfile/tistory/2260BC4C57C2D02512)

그래프의 중심점이 이동하는 모습을 볼 수 있다.

<br>

<br>

#### <u>Applying multivariate Gaussian distribution to anomaly detection</u>

지금까지 모델링 할 수 있는 여러가지 분포를 알아보았다.

이제 이러한 것들을 다른 이상 탐지 알고리즘에 적용하는 방법을 알아보자.

<br>

<b>Anomaly detection algorithm with multivariate Gaussian distribution</b>

data set을 가져와서 공식을 이용해 μ와 Σ를 계산한다.

새로운 예제가 주어진다.

![](http://www.holehouse.org/mlclass/15_Anomaly_Detection_files/Image%20[32].png)

Multivariate 가우스 분포를 이용하여 새로운 예제의 p(x)를 계산한다.

![](http://www.holehouse.org/mlclass/15_Anomaly_Detection_files/Image%20[33].png)

계산한 p(x)의 값과 ε값을 비교한다.

Multivariate 가우스 모델은 다음과 같다.

![img](http://www.holehouse.org/mlclass/15_Anomaly_Detection_files/Image%20[34].png)

녹색의 x를 이상 데이터라고 해석할 수 있다.

Original 가우스 모델은 multivariate 모델로도 설명할 수 있다.

![](http://www.holehouse.org/mlclass/15_Anomaly_Detection_files/Image%20[35].png)

위 그림과 같이 Σ의 값이 비대각성분이 모두 0이라면 original 모델과 같다.

<br>

<b>Original model vs. Multivariate Gaussian</b>

- Original 가우스 모델
  - 보편적으로 많이 사용된다.
  - 기존의 feature를 이용해서 새로운 feature를 생성해주어야한다.
  - 사람이 직접 새로운 feature를 생성해주면 error analysis를 통해서 문제를 분석하고 이것을 해결하는 과정이 필요하다.
  - 연산 비용이 적게 들어서 아주 많은 데이터를 처리하는데 용이하다.
  - training set이 작은 경우도 잘 동작한다.

- Multivariate 가우스 모델
  - 자주 사용되기는 하지만 특정한 조건에서만 사용을 할 수 있다.
  - original 모델에서 수작업으로 했던 new feature를 생성하는것을 고려하지 않아도 된다.
  - 하지만 Σ의 inverse를 계산해야 하기 때문에 연산 비용이 크다.
  - 반드시 m이 n보다 10배정도 큰 data set에서만 사용 가능하다.
  - inverse 연산을 해야하기 때문에 Σ가 non-invertible이면 안된다.
