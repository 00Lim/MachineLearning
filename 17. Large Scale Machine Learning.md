## Large Scale Machine Learning

#### <u>Learning with large datasets</u>

요즘 머신러닝이 많이 발전하는 이유는 예전보다 데이터가 많아졌기 때문이다.

하지만 데이터가 많아짐에 따라 더 많은 계산량이 필요하다.

<br>

<b>Why large datasets?</b>

![](http://www.holehouse.org/mlclass/17_Large_Scale_Machine_Learning_files/Image.png)

위 그래프는 데이터의 양과 훈련된 모델의 정확성의 관계를 나타낸 그래프이다.

데이터가 많으면 많을수록 알고리즘의 성능이 더 좋아진다.

하지만 데이터가 많아질수록 계산 문제가 생긴다.

<br>

<b>Learning with large datasets</b>

100,000,000개의 데이터로 gradient descent를 수행한다고 하자.

그렇다면 θ를 한 번 연산할 때 마다 100,000,000번의 계산을 해야 한다.

굳이 이렇게 많은 데이터를 가지고 학습을 해야할까?

랜덤으로 뽑은 1,000개의 데이터만으로도 충분히 학습이 가능하면 모든 데이터를 사용하지 않아도 된다.

이는 learning curve를 그려서 확인할 수 있다.

![](http://www.holehouse.org/mlclass/17_Large_Scale_Machine_Learning_files/Image%20[2].png)

이런 모양이라면 이것은 high variance 문제이므로 데이터를 더 넣어주어서 해결할 수 있다.

![](http://www.holehouse.org/mlclass/17_Large_Scale_Machine_Learning_files/Image%20[3].png)

이러한 형태라면 high bias 문제이므로 데이터를 더 넣어줘봤자 성능이 향상되지 않는다.

그렇다면 feature를 추가하거나 NN의 경우 hidden unit를 추가해서 문제를 해결할 수 있다.

<br>

<br>

#### <u>Stochastic Gradient Descent</u>

대부분의 학습은 최적화 목표(cost function)를 제시하고 알고리즘을 사용하여 그 값을 최소화 하는 방향으로 진행하였다.

하지만 data set이 큰 경우에는 gradient descent의 계산량이 너무 많아지기 때문에 다른 방법으로 최적화를 진행한다.

우리가 지금까지 배웠던 gradient descent는 batch gradient descent이다.

이제 배울 것은 stochastic gradient descent이다.

<br>

<b>Stochastic gradient descent</b>

cost function

![](http://www.holehouse.org/mlclass/17_Large_Scale_Machine_Learning_files/Image%20[9].png)

전체 cost function은 다음과 같다.

![](http://www.holehouse.org/mlclass/17_Large_Scale_Machine_Learning_files/Image%20[10].png)

이것은 batch gradient descent와 비슷하지만 살짝 다르다.

1. 무작위 셔플
2. 세타에 대한 편미분을 반복적으로 수행(summation 없음)

전체 데이터셋을 모두 연산하지 않고 하나의 데이터씩 순차적으로 연산하고 업데이트 한다.

![](http://www.holehouse.org/mlclass/17_Large_Scale_Machine_Learning_files/Image%20[12].png)

이와 같은 움직임을 표현하면 다음과 같다.

![](http://www.holehouse.org/mlclass/17_Large_Scale_Machine_Learning_files/Image%20[16].png)

이렇게 이동을 하면 한번에 global minimum으로 이동하지는 못하지만 돌고 돌아서 이동하게 된다.

이렇게 이동을 하더라도 batch gradient descent보다는 빠르게 찾게 된다.

<br>

<br>

#### <u> Mini-Batch Gradient Descent</u>

Mini-batch는 batch와 stochastic의 중간 단계이다.

Batch는 각 반복마다 모든 data set들을 연산하는 것이고 stochastic은 각 반복마다 1개의 data만 연산한다.

Mini-batch는 각 반복마다 data를 조금씩 묶어서(예: 10개씩, b = 10) 연산을 한다.

<br>

<b>Mini-batch algorithm</b>

![](http://www.holehouse.org/mlclass/17_Large_Scale_Machine_Learning_files/Image%20[17].png)

여기서 b와 m이 같다면 batch gradient descent와 동일하게 수행한다.

<br>

<b>Mini-batch gradient descent vs. stochastic gradient descent</b>

Mini-batch gradient descent

- 벡터화된 구현 가능
- 구현이 훨씬 효율적
- 계산을 부분적으로 병렬화하기 때문에 더 빠르다.

Mini-batch gradient descent의 단점

- 매개변수 b의 최적화가 필요

<br>

<br>

#### <u>Stochastic gradient descent convergence</u>

<b>Checking for convergence</b>

batch gradient descent와는 달리 stochastic 방식에서는 데이터를 합산하기 위해 알고리즘을 멈추지 않아도 된다.

Stochastic에서 cost가 잘 감소하고 있는지 중간에 확인할 수 있다.

![](http://www.holehouse.org/mlclass/17_Large_Scale_Machine_Learning_files/Image%20[20].png)

위 그래프는 learning rate에 따른 수렴값의 변화이다.

더 큰 learning rate를 사용할수록 수렴하지 못하고 global minimum을 중심으로 진동하기 때문에 learning rate가 작을수록 더 나은 결과를 얻을 수 있다.

![img](http://www.holehouse.org/mlclass/17_Large_Scale_Machine_Learning_files/Image%20[21].png)

위 그래프는 몇번마다 확인했는지 나타낸다.

파란색 그래프는 1000번 마다 한번씩 확인을 했고 빨간색은 5000번마다 확인했다.

빨간색 그래프가 조금 더 부드럽게 보이고 잘 수렴하고 있는것 같다.

하지만 빨간색 그래프가 항상 좋은 것은 아니다. 파란색 그래프에 비해 피드백이 덜 일어나기 때문에 모니터링이 느리게 될 수 있다.

![](http://www.holehouse.org/mlclass/17_Large_Scale_Machine_Learning_files/Image%20[22].png)

이런 그래프를 볼 수도 있다.

이럴때 파란색 그래프는 데이터셋에 노이즈가 많은것일수도 있으므로 더 많은 예제의 평균으로 살펴본다.

![](http://www.holehouse.org/mlclass/17_Large_Scale_Machine_Learning_files/Image%20[23].png)

이런 그래프의 경우에는 더 작은 learning rate를 사용해야 한다.

<br>

<b>Learning rate</b>

반복을 할수록 learning rate를 천천히 낮추는 방법이 있다.

- α = 상수1 / (반복 횟수 + 상수2)

이렇게 하면 반복을 하면 할수록 learning rate가 점점 작아진다.

![](http://www.holehouse.org/mlclass/17_Large_Scale_Machine_Learning_files/Image%20[24].png)

이렇게 learning rate를 잘 조정하면 global minimum에 가까워졌을 때 안정적으로 도달하게 된다.

<br>

<br>

#### <u>Online learning</u>

Online learning은 데이터가 지속적으로 업데이트 되는 것을 처리하기 위한 모델이다.

느린 업데이트를 한다는 점에서 gradient descent와 비슷하다.

고정된 데이터가 아닌 계속 변하는 데이터들을 처리한다.

<br>

예를 들어 배달 서비스가 있다.

배송 업체가 고객에게 출발지와 도착지 정보를 받으면 그에 따른 배송비를 책정하여 고객에게 전달한다.

고객은 배송비를 보고 배달 서비스를 이용할 것인지 아닌지를 판단한다.(이용하면 y = 1, 그렇지 않으면 y = 0)

그리고 x, y는 한 번 학습하고 버려진다. 새로운 데이터들이 계속 들어오기 때문이다.

<br>

<br>

#### <u>Map reduce and data parallelism</u>

데이터가 너무 커서 하나의 머신에서 학습하기 어려운 경우에 어떻게 할까?

이런 문제를 해결하기 위해 나온 것이 map reduce이다.

Map reduce로 여러 머신에서 동시에 학습을 진행 할 수 있기 때문에 더 효율이 좋다.

예를 들어 m이 400인 data set을 가지고 있다.

이것을 한 번에 연산하는 것이 아니라 100개씩 4개의 컴퓨터로 나눠서 계산하고 최종적으로 하나의 master 컴퓨터에서 통합시켜서 처리하는 방법이다.

![](http://www.holehouse.org/mlclass/17_Large_Scale_Machine_Learning_files/Image%20[28].png)

위와 같이 각각의 컴퓨터에서 계산한 결과를 temp 변수에 저장한다.

![](http://www.holehouse.org/mlclass/17_Large_Scale_Machine_Learning_files/Image%20[30].png)

그 다음 master 컴퓨터에서 이 변수들을 받아 더한다.

![](https://t1.daumcdn.net/cfile/tistory/245FD53557CAF5FF08)

위와 같이 변수들을 모두 더하는 식이 있다.

이런식으로 수행을 하면 나누지 않고 통째로 수행할 때보다 4배 가까이 속도가 향상된다.







































