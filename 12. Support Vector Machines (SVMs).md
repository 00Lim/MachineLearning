## Support Vector Machines

#### <u>Support Vector Machine (SVM) - Optimization objective</u>

Support vector machine은 다른 종류의 supervised learning algorithm으로, logistic regression보다 강력할 때가 있다.

<br>

<b>An alternative view of logistic regression</b>

이전과 마찬가지로 logistic regression의 hypothesis function은 다음과 같다.
$$
h_{\theta}(x) = \frac{1}{1 + e^{-{\theta}Tx}}
$$
우리는 이 함수를 조금 변형한 piecewise-linear한 함수를 SVM에서 이용할 것이다. 이 함수를 hinge loss function이라고 부른다.

이전에 logistic regression에서 사용하던 cost function은 다음과 같다.

![](http://www.holehouse.org/mlclass/12_Support_Vector_Machines_files/Image%20[4].png)

이 식은 y = 0일때와 y = 1일때 식이 두 부분으로 나뉘어져서 두 식을 하나로 합하여 사용했다.

y = 1일때의 cost function은

![](http://www.holehouse.org/mlclass/12_Support_Vector_Machines_files/Image%20[5].png)

y = 0일때의 cost function은

![](http://www.holehouse.org/mlclass/12_Support_Vector_Machines_files/Image%20[6].png)

위와 같이 그려지게 된다.

<br>

<b>SVM cost functions from logistic regression cost functions</b>

SVM에서의 cost function은 logistic regression과 비슷하지만 조금 다르다.

y = 1일때의 cost function은 아래의 분홍색 선과 같다.

![](http://www.holehouse.org/mlclass/12_Support_Vector_Machines_files/Image%20[7].png)

비슷하게 y = 0일때의 cost function도 아래의 분홍색 선과 같이 그려진다.

![](http://www.holehouse.org/mlclass/12_Support_Vector_Machines_files/Image%20[8].png)

<br>

<b>The complete SVM cost function</b>

이 두 개의 cost function을 각각 cost<sub>1</sub>, cost<sub>0</sub>이라고 정의하면 SVM의 cost function은 다음과 같이 나타낼 수 있다.

![](http://www.holehouse.org/mlclass/12_Support_Vector_Machines_files/Image%20[10].png)

<br>

<b>SVM notation is slightly different</b>

위의 cost function 식을 살짝 변형해보자.

- 1/m 제거

  - cost function이 최소가 되는 θ의 값을 찾는것이다. 1/m의 값은 상수이기 때문에 이 것을 제거하여도 θ는 여전히 최솟값을 가진다.

- 람다를 변형한다.

  - training data set term = A
  - Regularization term = B

  - 현재는 A + lambda * B의 공식인데 이걸 C * A + B의 공식으로 바꾼다. 이 C는 1/lambda와 같다고 볼 수 있다.

따라서 최종 cost function은 아래와 같다.

![](http://www.holehouse.org/mlclass/12_Support_Vector_Machines_files/Image%20[11].png)

θ<sup>T</sup>x이 0보다 크거나 같으면 h<sub>θ</sub>(x) = 1,

그렇지 않으면 h<sub>θ</sub>(x) = 0이 된다.

<br>

<br>

#### <u>Large margin intuition</u>

때때로 SVM은 Large Margin Classifiers라고 불린다. 그 이유를 알아보도록 하자.

cost function을 다시 한번 살펴보면

![](http://www.holehouse.org/mlclass/12_Support_Vector_Machines_files/Image%20[12].png)

위와 같다. 이것을 보면 값을 판단하는 기준값들 사이에 틈이 생기는 것을 알 수 있다.

만약 C의 값이 엄청 크다면 CA + B의 값을 최소화 하기 위해 A = 0이 되여야 할 것이다.

- A = 0으로 만들기
  - y = 1인 경우
    - A를 0으로 만들기 위해 θ<sup>T</sup>x의 값이 1보다 크거나 같아야한다.
  - y = 0인 경우
    - A를 0으로 만들기 위해 θ<sup>T</sup>x의 값이 -1보다 작거나 같아야 한다.

따라서 위의 조건을 만족하게 되면 결국 식에서는 B의 값만 남게 된다.

그렇기 때문에 B의 값만 최적화 하면 된다.

![](http://www.holehouse.org/mlclass/12_Support_Vector_Machines_files/Image%20[13].png)

이 문제를 해결하게 되면 흥미로운 경계를 얻게 된다.

![](http://www.holehouse.org/mlclass/12_Support_Vector_Machines_files/Image%20[14].png)

위 경계에서 분홍색과 연두색 경계도 데이터를 분류하는데 문제는 없는 경계이다. 하지만 새로운 데이터가 들어왔을 때 더 잘 구분할 수 있는 경계는 검정색 경계이다.

그 이유는 data들 간에 margin이라고 불리는 decision boundary와 데이터간의 거리의 차이가 클 수록 더 좋은 classifier라고 할 수 있다.

![](http://www.holehouse.org/mlclass/12_Support_Vector_Machines_files/Image%20[15].png)

<br>

<br>

#### <u>Large margin classification mathematics (optional)</u>

<b>Vector inner products</b>

백터의 내적에 대해 알아보자.

u 벡터와 v벡터가 다음과 같다고 할 때

![](http://www.holehouse.org/mlclass/12_Support_Vector_Machines_files/Image%20[18].png)

u 벡터는 파란선, v 벡터는 분홍선으로 표현되어 있다. v 벡터를 u 벡터에 투영시킨다고 했을 때 아래의 빨간 선 처럼 나타낼 수 있다.

![](http://www.holehouse.org/mlclass/12_Support_Vector_Machines_files/Image%20[20].png)

이것은 벡터를 내적하는 한 가지 방법이다.

- u <sup>T</sup> v = p * ||u||
  - p * ||u|| = u<sub>1</sub>v<sub>1</sub> + u<sub>2</sub>v<sub>2</sub>

<br>

<b>SVM decision boundary</b>

![](https://wikidocs.net/images/page/4280/svm204.PNG)

![](https://wikidocs.net/images/page/4280/svm205.PNG)

괄호 안의 수식이 0이 되는 부분이 decision boundary가 된다. 그 부분에 0을 넣으면 최적화 문제는 다음과 같이 단순화 된다.

![](http://www.holehouse.org/mlclass/12_Support_Vector_Machines_files/Image%20[22].png)

문제를 단순화 하기 위해 θ<sub>0</sub> = 0으로 놓고 feature가 2개인 경우를 가정해보자.

매개변수가 두 개밖에 없으므로 함수를 단순화하면

![](http://www.holehouse.org/mlclass/12_Support_Vector_Machines_files/Image%20[23].png)

그리고 이것은 다음과 같이 나타낼 수 있다.

![](http://www.holehouse.org/mlclass/12_Support_Vector_Machines_files/Image%20[24].png)

괄호 안에 든 것이 ||θ||이므로 위의 식은 다음과 같이 나타낼 수 있다.

![](http://www.holehouse.org/mlclass/12_Support_Vector_Machines_files/Image%20[26].png)

따라서 SVM은 norm의 제곱을 최소화한다.

이것이 의미하는 바는 다음과 같다.

- y<sup>(i)</sup> = 1일때, θ<sup>T</sup>x<sup>(i)</sup> ≥ 1이고 p<sup>i</sup> dot ||θ|| ≥ 1
- y<sup>(i)</sup> = 0일때, θ<sup>T</sup>x<sup>(i)</sup> ≤ −1이고 p<sup>i</sup> dot ||θ|| ≤ -1

<br>

<br>

#### <u>Kernels - 1: Adapting SVM to non-linear classifiers</u>

![](http://www.holehouse.org/mlclass/12_Support_Vector_Machines_files/Image%20[40].png)

위와 같은 data set은 non-linear boundary가 필요하다.

여기에서 SVM classifier가 목표로 하는 것은 θ<sub>0</sub> + θ<sub>1</sub>x<sub>1</sub> + θ<sub>2</sub>x<sub>2</sub> + ... + θ<sub>4</sub>x<sub>1</sub><sup>2</sup> + θ<sub>5</sub>x<sub>2</sub><sup>2</sup> + ... ≥ 0 일때 y = 1이 되도록 하는 것이다.

이 때 feature x들을 조금 더 일반적인 형태로 표현하기 위해 f로 고쳐서 써보자.

θ<sub>0</sub> + θ<sub>1</sub>f<sub>1</sub> + θ<sub>2</sub>f<sub>2</sub> + θ<sub>3</sub>f<sub>3</sub> + ...

이와 같은 새로운 feature f들은 기존의 feature들을 변형해서 나온 것들이다. 이 f를 어떻게 구하면 non-linear dicision boundary가 되는지 알아보자.

![](http://www.holehouse.org/mlclass/12_Support_Vector_Machines_files/Image%20[41].png)

우리의 feature space에 임의의 landmark l<sup>(1)</sup>, l<sup>(2)</sup>, l<sup>(3)</sup>이 있다고 하자.

새로운 feature f는 원래 feature가 각 landmark에 얼마나 가까운지 나타내는 것으로 정의한다.

![](http://www.holehouse.org/mlclass/12_Support_Vector_Machines_files/Image%20[42].png)

이 f함수는 kernel이라고 하며 우리가 사용하는 것은 가우시안 커널이라고 한다. landmark에 가까운수록 큰 값을 출력한다.

<br>

<b>What does σ do?</b>

σ는 가우스 커널에서 랜드마크 주변의 가파른 정도를 나타낸다.

값이 클수록 랜드마크는 더 완만한 경사를 갖게 된다.

σ<sup>2</sup> = 3일때

![](http://www.holehouse.org/mlclass/12_Support_Vector_Machines_files/Image%20[47].png)

σ<sup>2</sup> ≤ 0.5일때

![](http://www.holehouse.org/mlclass/12_Support_Vector_Machines_files/Image%20[46].png)

<br>

다음 그림에서 non-linear boundary를 예측해보자.

![](http://www.holehouse.org/mlclass/12_Support_Vector_Machines_files/Image%20[48].png)

공식을 적용해보면 f1은 1에 가깝고 f2와 f3은 0에 가깝다.

따라서

- θ<sub>0</sub> + θ<sub>1</sub>f<sub>1</sub> + θ<sub>2</sub>f<sub>2</sub> + θ<sub>3</sub>f<sub>3</sub> = 0.5 >= 0

0.5는 0보다 크기때문에 1로 예측이 된다. 

결국에는 아래와 같은 boundary를 얻게 된다.

![](http://www.holehouse.org/mlclass/12_Support_Vector_Machines_files/Image%20[50].png)

<br>

<br>

#### <u>Kernels II</u>

<b>Choosing the landmarks</b>

랜드마크는 x의 값과 정확히 동일한 위치에 배치한다.

따라서 랜드마크의 개수는 data set의 개수만큼 만들어지게 된다.

f벡터는 f<sub>0</sub> = 1이 항상 추가 되기 때문에 m + 1의 크기를 갖는다.

<br>

<b>SVM training with kernels</b>

![](http://www.holehouse.org/mlclass/12_Support_Vector_Machines_files/Image%20[51].png)

이제 x 대신 f를 feature 벡터로 사용하여 최소화한다.

이 최소화 문제를 해결하면 SVM에 대한 매개변수를 얻을 수 있다.



다른 알고리즘에도 커널을 적용할 수 있다.

하지만 SVM은 다른 알고리즘들보다 훨씬 더 효율적이다.

SVM 매개변수 C에 대해 알아보자.

- C가 크면 - low bias, high variance -> over fitting
- C가 작으면 - hihg bias, low variance -> under fitting

SVM 매개변수 σ<sup>2</sup>에 대해 알아보자.

- σ<sup>2</sup>가 크면 - f의 그래프가 부드럽게 그려진다. -> high bias, low variance
- σ<sup>2</sup>가 작으면 - f의 그래프가 급격하게 그려진다. -> low bias, high variance

<br>

<br>

#### <u>SVM - implementation and use</u>

<b>Choosing a kernel</b>

커널은 여러 종류가 있지만 몇가지만 살펴보도록 하자.

- 가우시안 커널
  - 우리가 방금까지 이야기 한 커널이다.
  - 주로 n이 작고 m이 클때(큰 2차원 training set) 선택한다.
  - feature들의 값의 크기가 많이 차이 난다면 전처리를 통해 값을 비슷하게 만들어준다.
- 선형 커널
  - 커널을 사용할 수 없을 때 사용한다. (따라서 f 벡터가 없다.)
  - 주로 n이 크고 m이 작을 때 사용한다.

<br>

<b>Logistic regression vs. SVM</b>

Logistic regression과 SVM중 어떤 것을 사용하면 좋을지는 n과 m을 통해 결정한다.

- n이 m보다 크다면
  - logistic regression이나 SVM
- n이 작고 m이 중간이면
  - 가우시안 커널이 좋다.
- n이 작고 m이 크다면
  - 가우시안 커널은 속도가 느리므로 선형커널 SVM과 logistic regression을 사용한다.