## Regularization

#### <u>The problem of overfitting</u>

Feature의 수가 너무 많으면 hypothesis function이 너무 복잡해진다.

Hypothesis function이 training data에 지나치게 맞춰진 모델은 새로운 데이터를 예측하는 데 실패할 수 있다.

Training data에 지나치게 맞춰져서 일반적인 추세를 표현하지 못하는 문제를 overfitting이라고 한다.

<br>

##### Addressing overfitting

Overfitting을 피하기 위해서는...

- Feature를 너무 많이 사용하진 않는 것이 좋다.
  - Feature를 자동으로 골라주는 알고리즘이 있다.

Feature를 버리면 중요한 데이터가 손실될 수 있다. 따라서 feature를 버리기 아까울 때 regularization을 사용한다. Regularization은 feature의 수를 그대로 유지하면서 모델이 너무 복잡해지지 않게 약간의 제약을 가하는 방법이다.

<br>

#### <u>Cost function optimization for regularization</u>

- 모든 feature를 유지하되, 파라미터 세타의 크기를 작게 유지한다.
- 많은 feature가 y를 예측하는데 조금씩 기여하는 경우 유용하다.
- 여러개의 파라미터가 있을 때 일부 파라미터들을 작은 값으로 만들면 hypothesis function이 간단해지도록 할 수 있다.
- 만약 cost 함수 J의 뒤에 더해주는 람다의 값을 아주 큰 값으로 정해주면 세타의 파라미터들의 값이 모두 0이 되어버린다. 따라서 상수인 세타<sub>0</sub>만 남게 되는데 이것은 dataset에 적절하지 않은 underfit의 함수가 되어버린다. -> 적절한 람다 값을 정하는 것이 중요한다.
- 세타<sub>0</sub>은 상수값이기 때문에 regularization에 포함하지 않는다.

<br>

#### <u>Regularization linear regression</u>

Regularization은 feature 개수에 비해 data 수가 부족할 때 발생하는 non-invertibility 문제를 해결하는데도 도움이 된다.



