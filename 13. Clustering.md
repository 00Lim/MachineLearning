## Clustering

#### <u>Unsupervised learning - introduction</u>

지금까지는 supervised learning, 즉 training set의 label이 주어지는 경우에 대하여 알아보았다.

 unsupervised learning은 주어진 label이 없이 training 한다. 예시로 비슷한 특성을 가진 데이터끼리 묶는 clustering이 있다.

클러스터링이 도움이 되는 것들

- market segmentation
- social network analysis
- astronomical data analysis

<br>

<br>

#### <u>K-means algorithm</u>

<b>Overview</b>

다음 데이터를 두 개의 그룹으로 나누어보자.

![](http://www.holehouse.org/mlclass/13_Clustering_files/Image.png)

1. 무작위로 두개의 중심점을 정한다.
2. 각 데이터들을 가장 가까운 중심점과 한 그룹으로 묶는다.

![](http://www.holehouse.org/mlclass/13_Clustering_files/Image%20[1].png)

3. 각 중심점들을 데이터들의 평균으로 조정한다.

![](http://www.holehouse.org/mlclass/13_Clustering_files/Image%20[2].png)

4. 중심점들이 더이상 변하지 않을때까지 2~3 과정을 반복한다.

결과

![](https://t1.daumcdn.net/cfile/tistory/236DCB4C57B3190422)

<br>

<b>K-means for non-separated clusters</b>

종종 k-means clustering이 잘 나뉘어 지지 않은 데이터에 적용할 때도 있다.

![](http://www.holehouse.org/mlclass/13_Clustering_files/Image%20[5].png)

예를 들어 위와 같은 티셔츠 사이즈 데이터를 세 가지 크기(S, M, L)로 나누고 싶다고 하면 위 데이터에 k-means clustering을 적용할 수 있다.

따라서 다음과 같은 세 개의 그룹으로 나눌 수 있다.

![](http://www.holehouse.org/mlclass/13_Clustering_files/Image%20[6].png)

<br>

<br>

#### <u>K means optimization objective</u>

supervised learning에는 cost function을 최소화 하는 최적화 목표가 있다.

k-means clustering에서도 supervised learning과 같은 최적화 목표가 있다.

k-means clustering의 objective

- c<sup>i</sup> = i번째의 x데이터가 몇번째 index의 cluster에 속하는지
- μ<sub>k</sub> = 중심점 k
- μ<sub>c</sub><sup>i</sup> = x<sup>i</sup>데이터가 속해있는 cluster의 중심점

이것들을 사용하여 최적화 목표를 설정하면

![](http://www.holehouse.org/mlclass/13_Clustering_files/Image%20[7].png)

즉, 데이터와 중심점 사이의 거리의 제곱을 최소화 하는게 목표이다.

위 식을 distortion(또는 distortion cost function)이라고 한다.

k-means 알고리즘으로

- 중심점을 옮길 때 J를 최소화 하는 방향으로 움직인다.
  - 각 data에 가장 가까운 중심점을 찾는다.
  - 중심점을 아예 변경하지 않는다.
- 이동 단계
  - 이 단계는 J의 값을 최소화하는 μ의 값을 선택하는것이다.
- 그래서 알고리즘을 두 부분으로 나누고 있다.
  - 첫 번째 부분은 c 변수를 최소화한다.
  - 두 번째 부분은 J 변수를 최소화한다.

우리는 이러한 사항들을 사용하여 k-means 알고리즘을 디버깅 할 수 있다.

<br>

<b>Random initialization</b>

중심점을 초기화하는 방법

- 데이터 수 m보다 작은 수의 중심점을 설정한다.(k > m이면 문제 있음)

- k는 초기화 지점에 따라 다른 솔루션으로 수렴할 수 있다.

  - local optimum의 위험이 있다.

    ![](http://www.holehouse.org/mlclass/13_Clustering_files/Image%20[10].png)

local optimum에 수렴하는걸 해결하기 위해

- 무작위로 여러번 초기화 한 후 결과를 확인해본다.
- 동일한 결과가 많으면 global optimum을 나타낼 가능성이 높다.

<br>

<br>

#### <u>How do we choose the number of clusters?</u>

자동으로 선택하는건 좋은게 아니다.

일반적으로 데이터 분포를 보고 수동으로 선택한다.

사실 어떤 k가 적절한지에 대한 명확한 답이 없다.

![](https://wikidocs.net/images/page/4694/uns501.PNG)

위의 예시에서는 cluster를 2개로 나눈게 적당할까 아니면 4개로 나누는게 적당할까?

<br>

<b>Elbow method</b>

가장 적절한 k의 값을 고르는 방법이 있다.

k 개수에 따라 cost function J를 그려보았을 때 특정 k 이후로 cost가 거의 변하지 않는 elbow point가 있다면 그 k를 선택하는 것이 합리적일것이다.

![](https://wikidocs.net/images/page/4694/uns502.PNG)

하지만 이 방법이 항상 통하는 것은 아니다. 다음과 같이 J의 변화가 대체로 부드럽다면 elbow라고 부를만한 지점이 없다. 이 경우는 elbow method로 k를 정하는데에 무리가 있다.

![](https://wikidocs.net/images/page/4694/uns503.PNG)

<br>

<b>Another method for clustering K</b>

또 다른 방법으로는 우리가 알고자 하는 목적에 맞게 조절을 해보는 것이다.

티셔츠 사이즈를 구분하는 예제의 경우 3개 혹은 5개로 구분을 함으로서 목적에 맞도록 조절 할 수 있다.